#Prediction Assignment Writeup 
library(caret)
 
# Read in the training and test sets by using read.csv function. Because the initial data sets contain coloumns mostly filled with blank cells, missining values, and even possible white spaces, it would be desirable to exclude all of them from the data as they wont actually explain the variation among the observations. To this end, all of the blank cells and even cells that might include space (" ") will be interpreted as "NA" as "na.strings charactor vector" is applid to the data sets.

training <- read.csv(file="pml-training.csv", header=T, na.strings=c("NA",""," "))
dim(training)
[1] 19622   160
testing  <- read.csv(file="pml-testing.csv", header=T, na.strings=c("NA",""," "))
dim(testing)
[1]  20 160

# Second, all the NAs columns are found in the data and the names of those varaibles (V) are stored:
V_training <-  colnames(training[ , colSums(is.na(training)) == 0])
V_testing <-  colnames(testing [ , colSums(is.na(testing )) == 0])
  
# Once the variables of interest are identified, a new training and test sets are created using only those variables
New_training <- training[V_training]
dim (New_training)
[1] 19622    60
New_testing <- testing[V_testing]
dim (New_testing)
[1] 20 60 

# Next, all the six first columns that containing additional information about the observations are removed as they are not of great interesst for the prediction model:
Final_training <- New_training [,-(1:6)]
dim(Final_training)
[1] 19622    54
Final_testing <- New_testing [,-(1:6)]
dim(Final_testing)
[1] 20 54

# Having done all the above preprocessing, there are only 54 variables left in the data. Now, the training data is divided into a sub-training and sub-testing data sets where the percentage of training data that goes to sub-training data is 60% and the remainder goes to the sub-testing. The sub-testing data will then be used to test the model.
set.seed (555)
trainIndex = createDataPartition(Final_training$classe, p = 0.6, list = FALSE)
sub_training = Final_training[trainIndex, ]
sub_testing = Final_training[-trainIndex, ]

# Now the model is trained using ONLY the sub_training data set. Random forest function in caret is applied on the sub_training data where a 5 fold cross validation strategy is used to decrease the risk for overfitting. One could even apply a 10 fold cross validation strategy for such a big data set where the number of observations are many. Moreover, the number of trees in RF is set to 1000 to decrease the likelihood of overfitting and to increase its performance. I choose Random forest as the classifier of choice because it is one of the best in accuracy among current algorithms, it is fast and effective in handling large data sets with lots of variables and observations. It provides estimates on variables that have the most predictive power in the classification, etc.

trControl = trainControl(method = "cv", number = 5)
rf_Model <- train (sub_training$classe ~ ., data=sub_training, method= "rf", ntree = 1000, importance = TRUE, trControl = trControl)
rf_Model
Random Forest 

11776 samples
   53 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (5 fold) 

Summary of sample sizes: 9421, 9421, 9420, 9422, 9420 

Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
   2    0.9912534  0.9889347  0.002005139  0.002536211
  27    0.9949895  0.9936622  0.002314634  0.002927567
  53    0.9915079  0.9892577  0.001410292  0.001784763

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 27. 

# final model output: the out-of-bag (OOB) estimate of error rate is 0.35% which is quite low
rf_Model$finalModel

Call:
 randomForest(x = x, y = y, ntree = 1000, mtry = param$mtry, importance = TRUE) 
               Type of random forest: classification
                     Number of trees: 1000
No. of variables tried at each split: 27

        OOB estimate of  error rate: 0.35%
Confusion matrix:
     A    B    C    D    E class.error
A 3348    0    0    0    0 0.000000000
B    7 2269    2    1    0 0.004387889
C    0    8 2045    1    0 0.004381694
D    0    0   11 1918    1 0.006217617
E    0    1    0    9 2155 0.004618938

# Plot the important variables
varImpPlot(rf_Model$finalModel, sort = TRUE, scale=TRUE)
# Calculate and display the important variables
varImp(rf_Model, scale = TRUE)

 rf variable importance

  variables are sorted by maximum importance across the classes
  only 20 most important variables shown (out of 53)

                          A      B     C     D      E
num_window           60.797 100.00 98.84 66.87 70.706
pitch_forearm        54.625  66.86 87.17 57.92 58.667
roll_belt            68.308  80.39 64.34 73.49 74.392
magnet_dumbbell_z    74.777  56.78 68.49 57.62 49.758
magnet_dumbbell_y    60.025  63.52 71.89 63.05 53.777
pitch_belt           26.090  66.21 48.85 39.67 34.637
yaw_belt             30.867  49.75 49.42 61.29 41.515
roll_forearm         39.576  27.77 29.34 24.83 26.042
accel_forearm_x      13.014  28.24 22.22 37.89 35.500
accel_dumbbell_y     27.571  25.79 36.81 26.73 28.531
gyros_dumbbell_y     34.426  23.81 25.24 18.22 15.311
total_accel_dumbbell 10.713  21.78 14.34 26.39 25.615
roll_dumbbell        15.669  24.94 18.52 20.56 25.643
magnet_belt_z         9.127  21.34 16.40 24.43 17.380
accel_dumbbell_z     17.636  23.47 20.60 22.03 24.279
magnet_belt_y        12.057  23.61 19.80 17.39 16.580
yaw_dumbbell         11.494  23.57 11.69 14.96 15.041
magnet_forearm_z     21.657  23.40 20.42 19.78 19.267
roll_arm             11.262  22.98 12.64 16.85  9.829
magnet_belt_x         6.422  22.26 22.20 16.78 18.879

# Prediction on the cases in the sub_testing data set:
pred <- predict(rf_Model, newdata = sub_testing)
 
# Create a confusion matrix to provide a set of performance statistics. The RF accuracy on the test set is 0.9978 which is perfect.
confusionMatrix(pred, sub_testing$classe) 
 
 Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2230    3    0    0    0
         B    1 1515    5    1    0
         C    0    0 1363    5    0
         D    0    0    0 1280    1
         E    1    0    0    0 1441

Overall Statistics
                                          
               Accuracy : 0.9978          
                 95% CI : (0.9965, 0.9987)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9973          
 McnemarÂ´s Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9991   0.9980   0.9963   0.9953   0.9993
Specificity            0.9995   0.9989   0.9992   0.9998   0.9998
Pos Pred Value         0.9987   0.9954   0.9963   0.9992   0.9993
Neg Pred Value         0.9996   0.9995   0.9992   0.9991   0.9998
Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2842   0.1931   0.1737   0.1631   0.1837
Detection Prevalence   0.2846   0.1940   0.1744   0.1633   0.1838
Balanced Accuracy      0.9993   0.9985   0.9978   0.9976   0.9996

# Prediction Assignment Submission: Instructions Help Center:
# First copy and paste the provided function which can be used to create text files that have one character each with the predicted label for the corresponding problem ID

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

# Apply the machine learning algorithm that is built to each of the 20 test cases in the testing data set:
Pred_test <- predict(rf_Model, Final_testing)
Pred_test
 [1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E

pml_write_files(Pred_test)
 
